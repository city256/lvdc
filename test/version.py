import pandas as pdimport datetimeimport tensorflow as tfimport numpy as npimport randomfrom collections import dequeimport matplotlib.pyplot as pltfrom pytimekr import pytimekrtrain_csv = pd.read_csv('../test/train_1day.csv')test_csv = pd.read_csv('../test/testset.csv')# hyperparameterw1 = 1 # cost weightw2 = 0 # peak weightw3 = 0 # swtich weightclass Environment:    def __init__(self, data):        # 초기 설정 상수        self.data = data        self.battery_cap = 1000        self.conv_cap = 250        self.contract = 500        self.soc_min = 0.1        self.soc_max = 0.9        self.total_steps = len(data)        self.demand_cost = 8320  # 7220, 8320, 9810        self.cnd_state = [0]        self.episode_reward = 0        self.current_step = 0        self.current_date = self.data.loc[self.data['Unnamed: 0_x'] == self.current_step, 'date'].iloc[0]        self.demand_sum = 0        # 모델링 변수 설정        self.done = False        self.soc = 0 # 초기 SoC 설정 (충전 상태)        self.load = 0        self.pv = 0        self.price = 0        self.peak_time = 0        self.charge = 0        self.discharge = 0        self.ess_action = 0        self.grid = 0        self.peak = 0        self.switch_sum = 0        self.workday = 0        self.usage_cost = 0 # max(0, self.grid) * self.price        self.usage_sum = 0        self.total_cost = 0 #(self.peak * self.demand_cost + self.usage_sum) * 1.137    def reset(self):        """        환경을 초기 상태로 재설정하고, 초기 상태를 반환합니다.        """        self.current_step = 0        self.current_date = self.data.loc[self.data['Unnamed: 0_x'] == self.current_step, 'date'].iloc[0]        self.done = False        self.soc = 0.5  # 초기 SoC 설정        self.load = float(round(self.data.loc[self.data['Unnamed: 0_x'] == self.current_step, 'load'].iloc[0], 2))        self.pv = float(round(self.data.loc[self.data['Unnamed: 0_x'] == self.current_step, 'pv'].iloc[0], 2))        self.grid = self.load - self.pv        self.price, self.peak_time = calculate_price(self.current_date)        self.workday = check_workday(self.current_date)        self.demand_sum = 0        self.charge = 0        self.discharge = 0        self.ess_action = 0        self.peak = self.grid        self.switch_sum = 0        self.usage_cost = max(0, self.grid) * self.price        self.usage_sum = self.usage_cost        self.total_cost = (self.peak * self.demand_cost + self.usage_sum) * 1.137        self.cnd_state = [0]        self.episode_reward = 0        state = [self.soc, self.load, self.pv, self.grid, -self.total_cost, self.switch_sum, self.peak, self.peak_time, self.workday]        return state    def step(self, action):        """        주어진 액션에 따라 환경의 상태를 업데이트하고,        다음 상태, 보상, 에피소드 종료 여부를 반환합니다.        """        current_date = self.data.loc[self.data['Unnamed: 0_x'] == self.current_step, 'date'].iloc[0]        current_load = float(self.data.loc[self.data['Unnamed: 0_x'] == self.current_step, 'load'].iloc[0])        current_pv = float(self.data.loc[self.data['Unnamed: 0_x'] == self.current_step, 'pv'].iloc[0])        current_grid = current_load - current_pv + (action / 4)        current_soc = calculate_soc(self.soc, action, self.battery_cap)        current_price, current_peak_time = calculate_price(self.current_date)        current_usage_cost = current_price * max(self.grid, 0)        current_peak = calculate_peak(self.peak, current_load, current_pv, action, self.contract)        # 충방전량 저장        if action > 0:            current_charge = action / 4            current_discharge = 0        elif action < 0:            current_charge = 0            current_discharge = abs(action / 4)        else:            current_charge = 0            current_discharge = 0        self.charge = current_charge        self.discharge = current_discharge        self.ess_action = action        self.current_date = current_date        self.load = current_load        self.pv = current_pv        self.grid = current_grid        self.price = current_price        self.soc = current_soc        self.workday = check_workday(self.current_date)        self.peak = current_peak        self.usage_cost = current_usage_cost        self.usage_sum += current_usage_cost        self.switch_sum += calculate_switch(action, self.cnd_state)        self.demand_sum = self.peak * self.demand_cost        self.total_cost = (self.demand_sum + self.usage_sum) * 1.137        # 보상 계산        # 마지막 스텝의 reward 계산 및 step + 1        if self.current_step >= self.total_steps - 1:  # 마지막 스텝에서 종료            self.done = True            reward = -self.total_cost  # 에피소드가 끝나면 total_cost에 따라 패널티/보상            print('Done!!!')        else:            # 상태 업데이트            if action > 0:  # 충전                if current_peak_time == 0:  # 저렴한 시간대                    reward = current_usage_cost   # 충전 시 보상 증가                elif current_peak_time == 2:  # 비싼 시간대                    reward = - (current_usage_cost * 1.5)                else:                    reward = - (current_usage_cost)            elif action < 0:  # 방전                if current_peak_time == 2:  # 비싼 시간대                    reward = - (current_usage_cost * 0.5)  # 방전 시 손해 증가                elif current_peak_time == 0:  # 저렴한 시간대                    reward = - (current_usage_cost * 1.5)                else:                    reward = - (current_usage_cost)            else:                reward = - (current_usage_cost)            # grid가 음수일 때 이를 양수로 만드는 액션에 추가 보상            if self.load - self.pv < 0 and current_grid >= 0:                reward += 10 * (current_grid * current_price)  # grid가 양수로 변경되면 보상 추가                print('prevent reverse')            self.current_step += 1        self.episode_reward += reward        next_state = [self.soc, self.load, self.pv, self.grid, -self.total_cost, self.switch_sum, self.peak,                      self.peak_time, self.workday]        self.render(action)        return next_state, reward, self.done    def render(self, action):        # 현재 상태를 출력        print(f"Step: {self.current_step}")        print(f"[{self.current_date}] SoC: {round(self.soc*100,2)}%")        print(f"Load: {self.load}, PV: {self.pv}, Grid : {round(self.grid,2)}, Price: {self.price}")        print(f"Total: {round(self.total_cost,0)}, Demand: {round(self.demand_cost*self.peak,0)}, Usage: {round(self.usage_sum, 0)}")        print(f"Switch Sum: {self.switch_sum}, Peak: {self.peak}")        print(f'action: {action}')        print("-" * 40)def cost_normalize(data, min_val, max_val):    # 최대값과 최소값이 동일할 경우 0 반환    if max_val == min_val:        return 0    return (data - min_val) / (max_val - min_val)def switch_normalize(data, min_val, max_val):    # 최대값과 최소값이 동일할 경우 0 반환    if max_val == min_val:        return 0    return (data - min_val) / (max_val - min_val)def peak_normalize(data, min_val, max_val):    # 최대값과 최소값이 동일할 경우 0 반환    if max_val == min_val:        return 0    return (data - min_val) / (max_val - min_val)def z_score_normalize(data):    mean = np.mean(data)    std = np.std(data)    if std == 0:        return 0  # 표준편차가 0인 경우 모든 값이 동일하므로 0으로 반환    return (data - mean) / std# Min-max 정규화 함수def min_max_normalize(data):    min_val = np.min(data)    max_val = np.max(data)    if max_val == min_val:        return 0  # 최대값과 최소값이 같은 경우 0으로 반환    return (data - min_val) / (max_val - min_val)# 필요한 함수 정의 (앞서 정의된 함수들)def cost_normalize(data, min_val, max_val):    # 최대값과 최소값이 동일할 경우 0 반환    if max_val == min_val:        return 0    return (data - min_val) / (max_val - min_val)def switch_normalize(data, min_val, max_val):    # 최대값과 최소값이 동일할 경우 0 반환    if max_val == min_val:        return 0    return (data - min_val) / (max_val - min_val)def peak_normalize(data, min_val, max_val):    # 최대값과 최소값이 동일할 경우 0 반환    if max_val == min_val:        return 0    return (data - min_val) / (max_val - min_val)def z_score_normalize(data):    mean = np.mean(data)    std = np.std(data)    if std == 0:        return 0  # 표준편차가 0인 경우 모든 값이 동일하므로 0으로 반환    return (data - mean) / std# Min-max 정규화 함수def min_max_normalize(data):    min_val = np.min(data)    max_val = np.max(data)    if max_val == min_val:        return 0  # 최대값과 최소값이 같은 경우 0으로 반환    return (data - min_val) / (max_val - min_val)def check_workday(date_str):    date = datetime.datetime.strptime(date_str, '%Y-%m-%d %H:%M:00')    # 해당 날짜가 공휴일인지 확인    holidays = pytimekr.holidays(date.year)    if date.weekday() >= 5 or date.date() in holidays:        return 0  # 공휴일 또는 주말    else:        return 1  # 평일, 주중def calculate_soc(previous_soc, action, max_capacity):    soc = ((max_capacity * previous_soc) + (action/4))/1000    return max(0, min(1, soc))# def compute_reward(total_cost, peak, switch_sum, a=w1, b=w2, c=w3):#     total_cost = cost_normalize(total_cost, 0 , 10000000)#     peak = peak_normalize(peak, 0 , 500)#     switch_sum = switch_normalize(switch_sum, 0, 1151 * 2 - 1)##     reward = - (a * total_cost + b * peak + c * switch_sum)#     return rewarddef compute_cost_switch(cost, switch_sum, w):    cost = cost_normalize(cost, 0 , 1000000)    switch_sum = switch_normalize(switch_sum, 0, 957)    reward = - (w * cost + (1-w) * switch_sum)    return rewarddef compute_cost(total_cost, peak, a=0.5, b=0.5):    reward = - (a * total_cost + b * peak)    return rewarddef calculate_peak(grid_prev, load, pv, action, P_contract):    max_P_grid = max(grid_prev, load - pv + action)  # grid의 최대값 계산    if max_P_grid > P_contract * 0.3:        return max_P_grid    else:        return P_contract * 0.3def calculate_switch(ess, cnd_state):    if ess > 0:        switch =  1    elif ess < 0:        switch = -1    else:        switch = 0    # 이전 상태와 현재 상태 비교    previous_state = cnd_state[-1]    if (previous_state == 1 and switch == -1) or (previous_state == -1 and switch == 1):        switch_value = 2  # 충전에서 방전으로, 방전에서 충전으로 전환    elif previous_state == switch:        switch_value = 0  # 상태 변화 없음    else:        switch_value = 1  # 대기 상태로의 전환 또는 대기에서 충전/방전으로 전환    cnd_state.append(switch)    #print(cnd_state[current_step], cnd_state[current_step-1],cnd_state[current_step] - cnd_state[current_step - 1] )    return switch_valuedef calculate_price(datetime_str):    dt = datetime.datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')    dt_15m_ago = dt - datetime.timedelta(minutes=15)    month = dt_15m_ago.month    hour = dt_15m_ago.hour    if 6 <= month <= 8: # 여름철        if 22 <= hour or hour <= 8: # 경부하            return 94, 0        elif 8 <= hour <= 11 or 12 <= hour <= 13 or 18 <= hour <= 22: # 중간부하            return 146.9, 1        else: # 최대부하            return 229, 2    elif month in [11, 12, 1, 2]: # 겨울철        if 22 <= hour or hour <= 8:  # 경부하            return 101, 0        elif 8 <= hour <= 9 or 12 <= hour <= 16 or 19 <= hour <= 22:  # 중간부하            return 147.1, 1        else:  # 최대부하            return 204.6, 2    else :  # 봄, 가을철        if 22 <= hour or hour <= 8:  # 경부하            return 94, 0        elif 8 <= hour <= 11 or 12 <= hour <= 13 or 18 <= hour <= 22:  # 중간부하            return 116.5, 1        else:  # 최대부하            return 147.2, 2class DQNAgent:    def __init__(self, state_size, action_size, env):        self.state_size = state_size        self.action_size = action_size        self.memory = deque(maxlen=2000)  # 메모리 크기를 늘림        self.gamma = 0.95        self.epsilon = 0.8        self.epsilon_min = 0.01        self.epsilon_decay = 0.995  # 더 느린 이플실론 감소율        self.learning_rate = 0.001        self.env = env        self.model = self._build_model()        self.target_model = self._build_model()        self.update_target_model()    def _build_model(self):        model = tf.keras.Sequential()        model.add(tf.keras.layers.Input(shape=(self.state_size,)))        model.add(tf.keras.layers.Dense(64, activation='relu'))  # 더 깊은 네트워크        model.add(tf.keras.layers.Dense(64, activation='relu'))        model.add(tf.keras.layers.Dense(32, activation='relu'))        model.add(tf.keras.layers.Dense(self.action_size, activation='tanh'))        model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))        return model    def update_target_model(self):        self.target_model.set_weights(self.model.get_weights())    def act(self, state):        if np.random.rand() <= self.epsilon:            # [-1, 1] 범위 내에서 랜덤 액션 선택            action = np.random.uniform(-1, 1) * 250  # [-250, 250] 범위로 조정            #action = np.random.uniform(-1, 1) * self.env.peak - self.env.grid  # peak 범위로 조정            #print('random : ',self.env.peak - self.env.grid)        else:            act_value = self.model.predict(state, verbose=None)[0][0]            action = np.clip(act_value * 250, -250, 250)  # [-250, 250] 범위로 조정        action = self.apply_constraints(state, action)        print(f'actual action : {action/4}')        return round(action, 0)    def apply_constraints(self, current_state, action):        soc, load, pv, grid, total_cost, switch_sum, peak, peak_time, workday = current_state[0]        # 현재 스텝의 다음 스텝 데이터를 가져옴        next_step = self.env.current_step        next_load = float(self.env.data.loc[self.env.data['Unnamed: 0_x'] == next_step, 'load'].iloc[0])        next_pv = float(self.env.data.loc[self.env.data['Unnamed: 0_x'] == next_step, 'pv'].iloc[0])        # SoC 제약 조건 적용        max_charge = min(self.env.conv_cap, (self.env.soc_max - soc) * self.env.battery_cap)        max_discharge = min(self.env.conv_cap, (soc - self.env.soc_min) * self.env.battery_cap)        # 충전/방전 후 예상 SoC 계산        expected_soc = soc + (action / 4) / self.env.battery_cap  # action은 kW 단위, SoC는 비율        expected_grid = next_load - next_pv        expected_grid_action = expected_grid + (action / 4)        print(f'First action: {action}, soc:{soc}/{expected_soc}, next_grid:{expected_grid}/{expected_grid_action}')        # 역송 발생 시 충전으로 전환, 이때 SoC를 고려하여 충전량 제한        if expected_grid < 0:            if soc <= self.env.soc_max:                action = min(max_charge, (-4 * expected_grid) * 1.1)  # 역송된 만큼 충전                expected_soc = soc + (action / 4) / self.env.battery_cap                expected_grid_action = expected_grid + (action / 4)                print(f'Natural reverse: {action}, soc:{soc}/{expected_soc}, next_grid:{expected_grid_action}')            else:  # 충전 용량 없으면 대기                action = 0                expected_soc = soc + (action / 4) / self.env.battery_cap                expected_grid_action = expected_grid + (action / 4)                print(f'Natural reverse, cant charge: {action}, soc:{soc}/{expected_soc}, next_grid:{expected_grid_action}')        else:            # 랜덤 액션으로 인한 역송 발생 ==> soc_min 까지 방전 or 대기            if expected_grid_action < 0:                if expected_soc <= self.env.soc_min: # 예상 soc가 10% 이하일때 soc_min 까지만 방전                    # SoC가 soc_min이 될 정도까지만 방전하도록 조정                    max_safe_discharge = (soc - self.env.soc_min) * self.env.battery_cap * 4                    action = max(-max_safe_discharge * 0.9,-max_discharge, (-4 * expected_grid)*0.9)                    expected_soc = soc + (action / 4) / self.env.battery_cap                    expected_grid_action = expected_grid + (action / 4)                    print(                        f'reverse action: {action}, soc:{soc}/{expected_soc}, next_grid:{expected_grid_action}')                else: # soc <= soc_max                    # SoC가 soc_min 이하라면 더 이상 방전하지 않고 대기                    action = max(-(4 * expected_grid)*0.9, -max_discharge)                    expected_soc = soc                    expected_grid_action = expected_grid + (action / 4)                    print(                        f'reverse cant action: {action}, soc:{soc}/{expected_soc}, next_grid:{expected_grid_action}')            else:                # SoC가 최소값에 도달할 경우 방전량을 제한ㄱ                if expected_soc < self.env.soc_min:                    required_action = (self.env.soc_min - soc) * self.env.battery_cap * 4                    action = max(required_action, action)                    expected_soc = soc + (action / 4) / self.env.battery_cap                    expected_grid_action = expected_grid + (action / 4)                    print(f'Adjusted action to maintain soc_min: {action}, soc:{soc}/{expected_soc}, next_grid:{expected_grid_action}')                elif expected_soc > self.env.soc_max:                    required_action = (self.env.soc_max - soc) * self.env.battery_cap * 4                    action = min(required_action, action)                    expected_soc = soc + (action / 4) / self.env.battery_cap                    expected_grid_action = expected_grid + (action / 4)                    print(                        f'Adjusted action to maintain soc_max: {action}, soc:{soc}/{expected_soc}, next_grid:{expected_grid_action}')                else:                    action = max(-max_discharge, min(action, max_charge))                    expected_soc = soc + (action / 4) / self.env.battery_cap                    expected_grid_action = expected_grid + (action / 4)                    print(f'meaningful action:{action}, soc:{soc}/{expected_soc}, next_grid:{expected_grid_action}')        # 작은 액션 제거 (노이즈 방지)        if -20 < action < 20:            action = 0            print("Action is too small, setting to 0.")        expected_soc = soc + (action / 4) / self.env.battery_cap        expected_grid_action = expected_grid + (action / 4)        print(            f'Load/PV: {next_load}/{next_pv}, Next grid: {expected_grid}, Grid with act: {expected_grid_action}, Expected SoC: {expected_soc}')        return action    # def apply_constraints(self, current_state, action, env):    #     soc, load, pv, grid, total_cost, switch_sum, peak = current_state    #    #     # SoC 제약 조건 적용    #     max_charge = min(env.conv_cap, (env.soc_max - soc) * env.battery_cap)    #     max_discharge = min(env.conv_cap, (soc - env.soc_min) * env.battery_cap)    #    #     # 예를 들어, 역송 조건에서만 충전하지 않도록 제약을 완화할 수 있습니다    #     if soc < env.soc_max and action > 0:    #         action = min(action, max_charge)  # 충전 제약 조건    #     elif soc > env.soc_min and action < 0:    #         action = max(action, -max_discharge)  # 방전 제약 조건    #    #     # 작은 액션 제거 (노이즈 방지)    #     if -20 < action < 20:    #         action = 0    #    #     return action    def remember(self, state, action, reward, next_state, done):        self.memory.append((state, action, reward, next_state, done))    def replay(self, batch_size):        minibatch = random.sample(self.memory, batch_size)        for state, action, reward, next_state, done in minibatch:            target = reward            if not done:                target = (reward + self.gamma * np.amax(self.target_model.predict(next_state, verbose=None)[0]))            target_f = self.model.predict(state,verbose=None)            target_f[0][0] = target            self.model.fit(state, target_f, epochs=1, verbose=0)        if self.epsilon > self.epsilon_min:            self.epsilon *= self.epsilon_decay    def load(self, name):        self.model.load_weights(name)    def save(self, name):        self.model.save_weights(name)def plot_results(dates, load, pv, grid, action, soc):    plt.figure(figsize=(14, 8))    # Load, PV, Grid 값을 하나의 그래프에 플롯    plt.plot(dates, load, label="Load", color="blue")    plt.plot(dates, pv, label="PV", color="orange")    plt.plot(dates, grid, label="Grid", color="green")    # Action과 SoC는 오른쪽 y축을 사용하여 플롯    plt.twinx()    #plt.plot(dates, action, label="Action", color="red", linestyle='--')    plt.plot(dates, soc, label="SoC", color="purple", linestyle='--')    # 라벨 및 타이틀 설정    plt.xlabel("Date")    plt.ylabel("Values")    plt.title("Load, PV, Grid, Action, and SoC Over Time")    # 범례 추가    plt.legend(loc="upper left")    plt.show()# 모델 학습 및 저장 코드def train_and_save_model(train_csv):    episode_rewards = []    # 환경 설정    env = Environment(data=train_csv)    # 에이전트와 환경 설정    state_size = 9  # 상태 공간 크기    action_size = 1  # 연속적인 액션 공간이지만 네트워크는 하나의 출력을 가짐    agent = DQNAgent(state_size, action_size, env)    # 학습 루프    num_episodes = 1000  # 에피소드 수    batch_size = 64    for e in range(num_episodes):        state = env.reset()        state = np.reshape(state, [1, state_size])        total_reward = 0  # 에피소드별로 누적 보상을 초기화        for time in range(env.total_steps):            action = agent.act(state)  # 행동 선택            next_state, usage_reward, done = env.step(action)  # 환경에서 행동 수행            next_state = np.reshape(next_state, [1, state_size])            agent.remember(state, action, usage_reward, next_state, done)  # 경험 저장            state = next_state  # 상태 업데이트            total_reward += usage_reward  # 매 스텝마다 `usage_reward` 누적            if done:                # 에피소드 종료 시 `total_cost` 기반 추가 보상 계산 및 업데이트                total_cost_reward = - ((total_reward + env.demand_cost * env.peak) * 1.137 / 100000)                for idx in range(len(agent.memory)):                    state, action, _, next_state, done = agent.memory[idx]                    final_reward = total_cost_reward + agent.memory[idx][2]  # 사용 비용에 대한 보상에 추가                    agent.memory[idx] = (state, action, final_reward, next_state, done)                agent.update_target_model()  # 타겟 네트워크 업데이트                print(f"Episode: {e}/{num_episodes}, Total Reward: {total_reward}, Total Cost: {env.total_cost}, Epsilon: {agent.epsilon:.2f}")                break            if len(agent.memory) > batch_size:                agent.replay(batch_size)  # 리플레이에서 학습        episode_rewards.append(total_reward)        plt.plot(episode_rewards, label='Total Reward')        plt.xlabel('Episode')        plt.ylabel('Total Reward')        plt.title('Episode vs Total Reward')        plt.legend()        plt.show()        # 매 에피소드 종료 후 모델 저장 (원하는 경우)        agent.model.save(f'cost_dqn{e}.keras')def load_and_predict(test_csv, model_load_path, epsilon = 0):    new_env = Environment(data=test_csv)    # 에이전트 초기화 및 모델 로드    state_size = 9    action_size = 1    agent = DQNAgent(state_size, action_size, new_env)    agent.model = tf.keras.models.load_model(model_load_path)    # 추론 시 epsilon을 0으로 설정    agent.epsilon = epsilon    # 추론 및 결과 저장을 위한 리스트 초기화    dates = []    loads = []    pvs = []    grids = []    actions = []    socs = []    # 추론 및 결과 출력    state = new_env.reset()    state = np.reshape(state, [1, len(state)])  # 상태 크기 맞춤    done = False    total_reward = 0    while not done:        # 현재 상태의 데이터 저장        current_date = new_env.current_date        dates.append(current_date)        loads.append(new_env.load)        pvs.append(new_env.pv)        grids.append(new_env.grid)        socs.append(new_env.soc)        action = agent.act(state)  # 행동 선택 시 제약 조건 적용        actions.append(action)        next_state, reward, done = new_env.step(action)        next_state = np.reshape(next_state, [1, len(next_state)])  # 상태 크기 맞춤        total_reward += reward        state = next_state    print(f"Total Reward for the new data: {total_reward}")    # 데이터 시각화    plot_results(dates, loads, pvs, grids, actions, socs)def load_and_resume_training(model_load_path, start_episode, num_episodes, train_csv):    # 환경 설정    env = Environment(data=train_csv)    episode_rewards= []    # 에이전트와 환경 설정    state_size = 9  # 상태 공간 크기    action_size = 1  # 연속적인 액션 공간이지만 네트워크는 하나의 출력을 가짐    agent = DQNAgent(state_size, action_size, env)    batch_size = 64    # 저장된 모델 불러오기    agent.load(model_load_path)    # 학습 루프    for e in range(start_episode, start_episode + num_episodes):        state = env.reset()        state = np.reshape(state, [1, state_size])        total_reward = 0        for time in range(env.total_steps):            action = agent.act(state)            next_state, reward, done = env.step(action)            next_state = np.reshape(next_state, [1, state_size])            agent.remember(state, action, reward, next_state, done)            state = next_state            total_reward = reward            print(f'episode: {e}, step: {time}')            if done:                agent.update_target_model()                print(f"Episode: {e + 1}/{start_episode + num_episodes}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}")                break            if len(agent.memory) > batch_size:                agent.replay(batch_size)        # 모델 저장        agent.model.save(f'dqn_model{e}.keras')        # 에피소드가 끝난 후 누적 보상을 저장        episode_rewards.append(total_reward)        # 주기적으로 보상 곡선 업데이트        plt.plot(episode_rewards)        plt.xlabel('Episode')        plt.ylabel('Total Reward')        plt.title('Episode vs Total Reward')        plt.show()test_csv = pd.read_csv('0907_0911.csv')# 학습 및 모델 저장 실행#train_and_save_model(test_csv)# 추론 실행load_and_predict(test_csv, 'cost_dqn5.keras', 0.001)# 가중치를 불러와 학습을 이어가는 예시#load_and_resume_training('dqn_train49_v5.keras', 49, 100, test_csv)